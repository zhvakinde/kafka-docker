# Default values

# advertised.listeners=PLAINTEXT://localhost:9092
# alter.config.policy.class.name=
# alter.log.dirs.replication.quota.window.num=11
# alter.log.dirs.replication.quota.window.size.seconds=1
# authorizer.class.name =
# auto.create.topics.enable=true
# auto.include.jmx.reporter=true
# auto.leader.rebalance.enable=true
# background.threads=10
# broker.heartbeat.interval.ms=2000
# broker.id=1
# broker.id.generation.enable=true
# broker.rack=
# broker.session.timeout.ms=9000
# client.quota.callback.class=
# compression.type=producer
# connection.failed.authentication.delay.ms=100
# connections.max.idle.ms=600000
# connections.max.reauth.ms=0
# control.plane.listener.name=
# controlled.shutdown.enable=true
# controlled.shutdown.max.retries=3
# controlled.shutdown.retry.backoff.ms=5000
# controller.listener.names=CONTROLLER
# controller.quorum.append.linger.ms=25
# controller.quorum.election.backoff.max.ms=1000
# controller.quorum.election.timeout.ms=1000
# controller.quorum.fetch.timeout.ms=2000
# controller.quorum.request.timeout.ms=2000
# controller.quorum.retry.backoff.ms=20
# controller.quorum.voters=[1@kafka01:9093, 2@kafka02:9093]
# controller.quota.window.num=11
# controller.quota.window.size.seconds=1
# controller.socket.timeout.ms=30000
# create.topic.policy.class.name=
# default.replication.factor=1
# delegation.token.expiry.check.interval.ms=3600000
# delegation.token.expiry.time.ms=86400000
# delegation.token.master.key=
# delegation.token.max.lifetime.ms=604800000
# delegation.token.secret.key=
# delete.records.purgatory.purge.interval.requests=1
# delete.topic.enable=true
# early.start.listeners=
# fetch.max.bytes=57671680
# fetch.purgatory.purge.interval.requests=1000
# group.initial.rebalance.delay.ms=3000
# group.max.session.timeout.ms=1800000
# group.max.size=2147483647
# group.min.session.timeout.ms=6000
# initial.broker.registration.timeout.ms=60000
# inter.broker.listener.name=PLAINTEXT
# inter.broker.protocol.version=3.4-IV0
# kafka.metrics.polling.interval.secs=10
# kafka.metrics.reporters=[]
# leader.imbalance.check.interval.seconds=300
# leader.imbalance.per.broker.percentage=10
# listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
# listeners=PLAINTEXT://:9092,CONTROLLER://:9093
# log.cleaner.backoff.ms=15000
# log.cleaner.dedupe.buffer.size=134217728
# log.cleaner.delete.retention.ms=86400000
# log.cleaner.enable=true
# log.cleaner.io.buffer.load.factor=0.9
# log.cleaner.io.buffer.size=524288
# log.cleaner.io.max.bytes.per.second=1.7976931348623157E308
# log.cleaner.max.compaction.lag.ms=9223372036854775807
# log.cleaner.min.cleanable.ratio=0.5
# log.cleaner.min.compaction.lag.ms=0
# log.cleaner.threads=1
# log.cleanup.policy=[delete]
# log.dir=/tmp/kafka-logs
# log.dirs=/tmp/kraft-combined-logs
# log.flush.interval.messages=9223372036854775807
# log.flush.interval.ms=
# log.flush.offset.checkpoint.interval.ms=60000
# log.flush.scheduler.interval.ms=9223372036854775807
# log.flush.start.offset.checkpoint.interval.ms=60000
# log.index.interval.bytes=4096
# log.index.size.max.bytes=10485760
# log.message.downconversion.enable=true
# log.message.format.version=3.0-IV1
# log.message.timestamp.difference.max.ms=9223372036854775807
# log.message.timestamp.type=CreateTime
# log.preallocate=false
# log.retention.bytes=-1
# log.retention.check.interval.ms=300000
# log.retention.hours=168
# log.retention.minutes=
# log.retention.ms=
# log.roll.hours=168
# log.roll.jitter.hours=0
# log.roll.jitter.ms=
# log.roll.ms=
# log.segment.bytes=1073741824
# log.segment.delete.delay.ms=60000
# max.connection.creation.rate=2147483647
# max.connections=2147483647
# max.connections.per.ip=2147483647
# max.connections.per.ip.overrides =
# max.incremental.fetch.session.cache.slots=1000
# message.max.bytes=1048588
# metadata.log.dir=
# metadata.log.max.record.bytes.between.snapshots=20971520
# metadata.log.max.snapshot.interval.ms=3600000
# metadata.log.segment.bytes=1073741824
# metadata.log.segment.min.bytes=8388608
# metadata.log.segment.ms=604800000
# metadata.max.idle.interval.ms=500
# metadata.max.retention.bytes=104857600
# metadata.max.retention.ms=604800000
# metric.reporters=[]
# metrics.num.samples=2
# metrics.recording.level=INFO
# metrics.sample.window.ms=30000
# min.insync.replicas=1
# node.id=1
# num.io.threads=8
# num.network.threads=3
# num.partitions=1
# num.recovery.threads.per.data.dir=1
# num.replica.alter.log.dirs.threads=null
# num.replica.fetchers=1
# offset.metadata.max.bytes=4096
# offsets.commit.required.acks=-1
# offsets.commit.timeout.ms=5000
# offsets.load.buffer.size=5242880
# offsets.retention.check.interval.ms=600000
# offsets.retention.minutes=10080
# offsets.topic.compression.codec=0
# offsets.topic.num.partitions=50
# offsets.topic.replication.factor=1
# offsets.topic.segment.bytes=104857600
# password.encoder.cipher.algorithm=AES/CBC/PKCS5Padding
# password.encoder.iterations=4096
# password.encoder.key.length=128
# password.encoder.keyfactory.algorithm=
# password.encoder.old.secret=
# password.encoder.secret=
# principal.builder.class=class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
# process.roles=[broker, controller]
# producer.id.expiration.check.interval.ms=600000
# producer.id.expiration.ms=86400000
# producer.purgatory.purge.interval.requests=1000
# queued.max.request.bytes=-1
# queued.max.requests=500
# quota.window.num=11
# quota.window.size.seconds=1
# remote.log.index.file.cache.total.size.bytes=1073741824
# remote.log.manager.task.interval.ms=30000
# remote.log.manager.task.retry.backoff.max.ms=30000
# remote.log.manager.task.retry.backoff.ms=500
# remote.log.manager.task.retry.jitter=0.2
# remote.log.manager.thread.pool.size=10
# remote.log.metadata.manager.class.name=
# remote.log.metadata.manager.class.path=
# remote.log.metadata.manager.impl.prefix=
# remote.log.metadata.manager.listener.name=
# remote.log.reader.max.pending.tasks=100
# remote.log.reader.threads=10
# remote.log.storage.manager.class.name=
# remote.log.storage.manager.class.path=
# remote.log.storage.manager.impl.prefix=
# remote.log.storage.system.enable=false
# replica.fetch.backoff.ms=1000
# replica.fetch.max.bytes=1048576
# replica.fetch.min.bytes=1
# replica.fetch.response.max.bytes=10485760
# replica.fetch.wait.max.ms=500
# replica.high.watermark.checkpoint.interval.ms=5000
# replica.lag.time.max.ms=30000
# replica.selector.class=
# replica.socket.receive.buffer.bytes=65536
# replica.socket.timeout.ms=30000
# replication.quota.window.num=11
# replication.quota.window.size.seconds=1
# request.timeout.ms=30000
# reserved.broker.max.id=1000
# sasl.client.callback.handler.class=
# sasl.enabled.mechanisms=[GSSAPI]
# sasl.jaas.config=
# sasl.kerberos.kinit.cmd=/usr/bin/kinit
# sasl.kerberos.min.time.before.relogin=60000
# sasl.kerberos.principal.to.local.rules=[DEFAULT]
# sasl.kerberos.service.name=
# sasl.kerberos.ticket.renew.jitter=0.05
# sasl.kerberos.ticket.renew.window.factor=0.8
# sasl.login.callback.handler.class=
# sasl.login.class=
# sasl.login.connect.timeout.ms=
# sasl.login.read.timeout.ms=
# sasl.login.refresh.buffer.seconds=300
# sasl.login.refresh.min.period.seconds=60
# sasl.login.refresh.window.factor=0.8
# sasl.login.refresh.window.jitter=0.05
# sasl.login.retry.backoff.max.ms=10000
# sasl.login.retry.backoff.ms=100
# sasl.mechanism.controller.protocol=GSSAPI
# sasl.mechanism.inter.broker.protocol=GSSAPI
# sasl.oauthbearer.clock.skew.seconds=30
# sasl.oauthbearer.expected.audience=
# sasl.oauthbearer.expected.issuer=
# sasl.oauthbearer.jwks.endpoint.refresh.ms=3600000
# sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms=10000
# sasl.oauthbearer.jwks.endpoint.retry.backoff.ms=100
# sasl.oauthbearer.jwks.endpoint.url=
# sasl.oauthbearer.scope.claim.name=scope
# sasl.oauthbearer.sub.claim.name=sub
# sasl.oauthbearer.token.endpoint.url=
# sasl.server.callback.handler.class=
# sasl.server.max.receive.size=524288
# security.inter.broker.protocol=PLAINTEXT
# security.providers=
# socket.connection.setup.timeout.max.ms=30000
# socket.connection.setup.timeout.ms=10000
# socket.listen.backlog.size=50
# socket.receive.buffer.bytes=102400
# socket.request.max.bytes=104857600
# socket.send.buffer.bytes=102400
# ssl.cipher.suites=[]
# ssl.client.auth=none
# ssl.enabled.protocols=[TLSv1.2, TLSv1.3]
# ssl.endpoint.identification.algorithm=https
# ssl.engine.factory.class=
# ssl.key.password=
# ssl.keymanager.algorithm=SunX509
# ssl.keystore.certificate.chain=
# ssl.keystore.key=
# ssl.keystore.location=
# ssl.keystore.password=
# ssl.keystore.type=JKS
# ssl.principal.mapping.rules=DEFAULT
# ssl.protocol=TLSv1.3
# ssl.provider=
# ssl.secure.random.implementation=
# ssl.trustmanager.algorithm=PKIX
# ssl.truststore.certificates=
# ssl.truststore.location=
# ssl.truststore.password=
# ssl.truststore.type=JKS
# transaction.abort.timed.out.transaction.cleanup.interval.ms=10000
# transaction.max.timeout.ms=900000
# transaction.remove.expired.transaction.cleanup.interval.ms=3600000
# transaction.state.log.load.buffer.size=5242880
# transaction.state.log.min.isr=1
# transaction.state.log.num.partitions=50
# transaction.state.log.replication.factor=1
# transaction.state.log.segment.bytes=104857600
# transactional.id.expiration.ms=604800000
# unclean.leader.election.enable=false
# zookeeper.clientCnxnSocket=
# zookeeper.connect=
# zookeeper.connection.timeout.ms=10000
# zookeeper.max.in.flight.requests=10
# zookeeper.metadata.migration.enable=false
# zookeeper.session.timeout.ms=18000
# zookeeper.set.acl=false
# zookeeper.ssl.cipher.suites=
# zookeeper.ssl.client.enable=false
# zookeeper.ssl.crl.enable=false
# zookeeper.ssl.enabled.protocols=
# zookeeper.ssl.endpoint.identification.algorithm=HTTPS
# zookeeper.ssl.keystore.location=
# zookeeper.ssl.keystore.password=
# zookeeper.ssl.keystore.type=
# zookeeper.ssl.ocsp.enable=false
# zookeeper.ssl.protocol=TLSv1.2
# zookeeper.ssl.truststore.location=
# zookeeper.ssl.truststore.password=
# zookeeper.ssl.truststore.type=

# Overrides

# Dynamic overrides
node.id={{KAFKA_NODE_ID}}
broker.id={{KAFKA_BROKER_ID}}
process.roles={{KAFKA_ROLES}}
controller.quorum.voters={{KAFKA_QUORUM}}
listeners={{KAFKA_LISTENERS}}
advertised.listeners={{KAFKA_ADVERTISED_LISTENERS}}
listener.security.protocol.map={{KAFKA_LISTENERS_PROTOCOLS}}
num.partitions={{KAFKA_PARTITIONS}}
default.replication.factor={{KAFKA_DEFAULT_REPLICAS}}
transaction.state.log.replication.factor={{KAFKA_REPLICAS}}
transaction.state.log.min.isr={{KAFKA_MIN_ISR}}

# Static overrides
inter.broker.listener.name=PLAINTEXT
controller.listener.names=CONTROLLER
log.dirs=data
